{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import xgboost as xgb \n",
    "from sklearn.linear_model import  LogisticRegression, LinearRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt \n",
    "import mlxtend\n",
    "import imblearn \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import  cross_validate, cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment (Hyperopt parameter)\n",
    "- optimal estimator 126, max_depth rf = 8 (rf) \n",
    "- optimal estimator 228 , maxdepth = 8 (xgb)\n",
    "* Optimized model do not perform any better than default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed \n",
    "traindata = pd.read_csv('../doseprediction/data/sl_train.csv')\n",
    "testdata = pd.read_csv('../doseprediction/data/sl_val.csv')\n",
    "print(traindata.shape)\n",
    "print(testdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfeatures = ['Gender','Ventilator', 'diabetes', 'metastatic_cancer', 'qsofa_sysbp_score',\n",
    "       'qsofa_resprate_score', 'qsofa_gcs_score', \n",
    "        'Age', 'Weight', 'Height', 'bmi', 'ANION_GAP',\n",
    "       'APTT', 'Albumine', 'Art_BE', 'Art_PH',  'Bicarbonaat',\n",
    "       'Calcium', 'Chloride', 'DIA', 'FiO2', 'Glucose', 'HB', 'HeartRate',\n",
    "       'Ion_Ca', 'Kalium', 'LEU', 'Lactate', 'MAP', 'Magnesium', 'Natrium',\n",
    "       'PF_ratio', 'PaCO2', 'PaO2', 'RespRate', 'SYS', 'Shock_Index',\n",
    "       'Sirs_score', 'Sofa_score', 'Temp', 'Trombo', 'elixhauser',\n",
    "       'elixhauser_hospital', 'qsofa', 'mingcs', 'lods', \n",
    "       'SpO2', 'Ureum', 'Creat', 'ALAT',\n",
    "       'ASAT', 'Bili', 'INR' ]# 'blood_culture_positive','race_white', 'race_black', 'race_hispanic', 'race_other']\n",
    "\n",
    "fluidfeatures = ['cumm_fluid_balance','total_IV_prev', 'max_VP_prev', \n",
    "                            'Running_total_UP', 'total_UP']\n",
    "\n",
    "target = ['max_VP', 'total_IV', 'discrete_action', 'discrete_VP', \n",
    "          'discrete_IV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training predictors\n",
    "X_train = traindata[keyfeatures]\n",
    "X_test = testdata[keyfeatures]\n",
    "X_train_extra =traindata[keyfeatures+fluidfeatures]\n",
    "X_test_extra =testdata[keyfeatures+fluidfeatures]\n",
    "\n",
    "# training targets \n",
    "Y_train = traindata[target]\n",
    "Y_test = testdata[target]\n",
    "#binarize VP as it is highly imbalance \n",
    "Y_train['binary_VP']=  np.where(Y_train.discrete_VP==0, 0, 1)\n",
    "Y_test = testdata[target]\n",
    "Y_test['binary_VP']=  np.where(Y_test.discrete_VP==0, 0, 1)\n",
    "\n",
    "# Y all  for visualization \n",
    "Y = pd.concat([Y_train, Y_test])\n",
    "X = pd.concat([X_train, X_test])\n",
    "X_extra = pd.concat([X_train_extra, X_test_extra])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classification(model, x_train, y_train, x_test, y_test, modelname):\n",
    "    model.fit(x_train, y_train)\n",
    "    predict = model.predict(x_test)\n",
    "    predict_proba = model.predict_proba(x_test)\n",
    "    f1 = metrics.f1_score(y_test, predict)\n",
    "    accuracy = metrics.balanced_accuracy_score(y_test, predict)\n",
    "    gmean = imblearn.metrics.geometric_mean_score(y_test, predict)\n",
    "    logloss = metrics.log_loss(y_test, predict_proba)\n",
    "    report = metrics.classification_report(y_test, predict)\n",
    "    precision = metrics.precision_score(y_test, predict)\n",
    "    recall = metrics.recall_score(y_test, predict)\n",
    "    rocauc = metrics.roc_auc_score(y_test, predict_proba[:,1])\n",
    "    regular_accuracy = metrics.accuracy_score(y_test, predict)\n",
    "    eval = {'Model' : modelname,\n",
    "            'F1' : f1,\n",
    "            'Balanced_Accuracy' : accuracy,\n",
    "            'Geo_Mean' : gmean,\n",
    "            'log loss' : logloss,\n",
    "            'Precision' : precision,\n",
    "            'Recall' : recall,\n",
    "            'Regular_Accuracy' : regular_accuracy,\n",
    "            'AUCROC' : rocauc\n",
    "            }\n",
    "    #print(eval)\n",
    "    return model, pd.DataFrame(eval, index=[0])\n",
    "\n",
    "performance = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model logistic regression\n",
    "modelname = 'LR-base'\n",
    "model1 = LogisticRegression(random_state=112)\n",
    "model1, per= eval_classification(model1, X_train, Y_train.binary_VP, X_test, Y_test.binary_VP, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model Random forest Base\n",
    "modelname = 'RF-base'\n",
    "model2 = RandomForestClassifier(random_state=112, n_jobs=-1)\n",
    "model2, per= eval_classification(model2, X_train, Y_train.binary_VP, X_test, Y_test.binary_VP, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model XGB Base\n",
    "modelname = 'XGB-base'\n",
    "model3 = xgb.XGBClassifier(random_state=112, n_jobs=-1, eval_metric='logloss')\n",
    "model3, per= eval_classification(model3, X_train, Y_train.binary_VP, X_test, Y_test.binary_VP, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model LR base+\n",
    "modelname = 'LR-base+'\n",
    "model4 = LogisticRegression(random_state=112)\n",
    "model4, per= eval_classification(model4, X_train_extra, Y_train.binary_VP, X_test_extra, Y_test.binary_VP, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model Random forest Base+\n",
    "modelname = 'RF-base+'\n",
    "model5 = RandomForestClassifier(random_state=112, n_jobs=-1)\n",
    "model5, per= eval_classification(model5, X_train_extra, Y_train.binary_VP, X_test_extra, Y_test.binary_VP, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model XGB base +\n",
    "modelname = 'XGB-base+'\n",
    "model6 = xgb.XGBClassifier(random_state=112, n_jobs=-1, eval_metric='logloss')\n",
    "model6, per= eval_classification(model6, X_train_extra, Y_train.binary_VP, X_test_extra, Y_test.binary_VP, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "### Compare RF-base + (model 5) and XGB base + (model 6)  \n",
    "t,p = paired_ttest_5x2cv(estimator1=model5,\n",
    "                          estimator2=model6,\n",
    "                          X=X_extra, y=Y.binary_VP,\n",
    "                          random_seed=112, \n",
    "                          scoring=make_scorer(imblearn.metrics.geometric_mean_score))\n",
    "\n",
    "print('stat:', t)\n",
    "print('p-value:', p)\n",
    "\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0), \\\n",
    "    and may conclude that the performance of the two algorithms is not significantly different.')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0), model are significantly different')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare LR-base + (model 4) and XGB base + (model 6)  \n",
    "t,p = paired_ttest_5x2cv(estimator1=model4,\n",
    "                          estimator2=model6,\n",
    "                          X=X_extra, y=Y.binary_VP,\n",
    "                          random_seed=112, \n",
    "                          scoring=make_scorer(imblearn.metrics.geometric_mean_score))\n",
    "\n",
    "print('stat:', t)\n",
    "print('p-value:', p)\n",
    "\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0), \\\n",
    "    and may conclude that the performance of the two algorithms is not significantly different.')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0), model are significantly different')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macnemar test RF and XGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import mcnemar_table, mcnemar\n",
    "print(\"McNemar:\")\n",
    "y_model5 = model5.predict(X_test_extra)\n",
    "y_model6 = model6.predict(X_test_extra)\n",
    "\n",
    "        # Calculate p value\n",
    "tb = mcnemar_table(y_target=Y_test.binary_VP, \n",
    "                          y_model1=y_model5, \n",
    "                          y_model2=y_model6)\n",
    "        \n",
    "chi2, p = mcnemar(ary=tb, exact=True)\n",
    "\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0)-model are same')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0), significant difference in model performance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF confusion matrix\n",
    "metrics.confusion_matrix(Y_test.binary_VP, y_model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB confusion matrix\n",
    "metrics.confusion_matrix(Y_test.binary_VP, y_model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original data \n",
    "metrics.confusion_matrix(Y_test.binary_VP, Y_test.binary_VP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_cls_report = imblearn.metrics.classification_report_imbalanced(Y_test.binary_VP, y_model6)\n",
    "print(model6_cls_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_cls_report = imblearn.metrics.classification_report_imbalanced(Y_test.binary_VP, y_model5)\n",
    "print(model5_cls_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance and shap value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, x_test):\n",
    "    names=x_test.columns\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(model.feature_importances_)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "#Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "#Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    return fi_df\n",
    "\n",
    "def shapanalysis(model, x_test):\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(x_test)\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for XGBoost -base + model \n",
    "fi_model6 = get_feature_importance(model6, X_test_extra)\n",
    "shap_value = shapanalysis(model6, X_test_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot feature importance \n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=fi_model6['feature_importance'][0:15], \n",
    "y=fi_model6['feature_names'][0:15])\n",
    "plt.show()\n",
    "shap.summary_plot(shap_value, X_test_extra , max_display=15, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ROC Curve \n",
    "y_pred = model1.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test.binary_VP, y_pred)\n",
    "auc = round(metrics.roc_auc_score(Y_test.binary_VP, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"LR-base (basemodel), AUC=\"+str(auc))\n",
    "\n",
    "\n",
    "y_pred = model2.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test.binary_VP, y_pred)\n",
    "auc = round(metrics.roc_auc_score(Y_test.binary_VP, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"RF-base, AUC=\"+str(auc))\n",
    "\n",
    "y_pred = model3.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test.binary_VP, y_pred)\n",
    "auc = round(metrics.roc_auc_score(Y_test.binary_VP, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"XGB-base, AUC=\"+str(auc))\n",
    "\n",
    "y_pred = model4.predict_proba(X_test_extra)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test.binary_VP, y_pred)\n",
    "auc = round(metrics.roc_auc_score(Y_test.binary_VP, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"LR-base+, AUC=\"+str(auc))\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model5.predict_proba(X_test_extra)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test.binary_VP, y_pred)\n",
    "auc = round(metrics.roc_auc_score(Y_test.binary_VP, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"RF-base+, AUC=\"+str(auc))\n",
    "\n",
    "y_pred = model6.predict_proba(X_test_extra)[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test.binary_VP, y_pred)\n",
    "auc = round(metrics.roc_auc_score(Y_test.binary_VP, y_pred), 4)\n",
    "plt.plot(fpr,tpr,label=\"XGB-base+, AUC=\"+str(auc))\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "VP_models = {'LR-base' : model1,\n",
    "              'RF-base' : model2,\n",
    "              'XGB-base' : model3,\n",
    "              'LR-base+': model4,\n",
    "              'RF-base+': model5,\n",
    "              'XGB-base+' : model6\n",
    "              }\n",
    "VP_feature_imp = {'XGB-base+_shape' : shap_value,\n",
    "                  'XGB-base+_fi' : fi_model6}\n",
    "\n",
    "performance.to_csv('../doseprediction/Final_VP_model_Performances.csv')\n",
    "joblib.dump(VP_models, '../doseprediction/Final_VP_models.pkl')\n",
    "joblib.dump(VP_feature_imp,  '../doseprediction/Final_VP_Featureimp.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame()\n",
    "scoring = {'f1_score': make_scorer(metrics.f1_score, greater_is_better=True),\n",
    "           'log_loss' : make_scorer(metrics.log_loss),\n",
    "           'g_mean' : make_scorer(imblearn.metrics.geometric_mean_score, greater_is_better=True),\n",
    "           'b_accuracy' : make_scorer(metrics.balanced_accuracy_score, greater_is_better=True),\n",
    "        'precision': make_scorer(metrics.precision_score, greater_is_better=True),\n",
    "        'recall': make_scorer(metrics.recall_score, greater_is_better=True),\n",
    "        'reg_accuracy': make_scorer(metrics.accuracy_score, greater_is_better=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic base\n",
    "modelname = 'LR-base'\n",
    "lgfit = LogisticRegression()\n",
    "lg_results = cross_validate(lgfit, X, Y.binary_VP, cv=5,\n",
    "                         scoring=scoring)\n",
    "lg_results = pd.DataFrame.from_dict(lg_results)\n",
    "results = pd.DataFrame()\n",
    "results['mean'] = lg_results.drop(['fit_time', 'score_time'], axis=1).mean()\n",
    "results['se'] = lg_results.drop(['fit_time', 'score_time'], axis=1).sem()\n",
    "results = results.T\n",
    "results['model'] = modelname\n",
    "#cv_results = pd.concat([cv_results, results])\n",
    "cv_results =cv_results.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic base+\n",
    "modelname = 'LR-base+'\n",
    "lgfit = LogisticRegression()\n",
    "lg_results = cross_validate(lgfit, X_extra, Y.binary_VP, cv=5,\n",
    "                         scoring=scoring)\n",
    "lg_results = pd.DataFrame.from_dict(lg_results)\n",
    "results = pd.DataFrame()\n",
    "results['mean'] = lg_results.drop(['fit_time', 'score_time'], axis=1).mean()\n",
    "results['se'] = lg_results.drop(['fit_time', 'score_time'], axis=1).sem()\n",
    "results= results.T\n",
    "results['model'] = modelname\n",
    "#cv_results = pd.concat([cv_results, results])\n",
    "cv_results =cv_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf-base\n",
    "modelname = 'RF-base'\n",
    "lgfit = RandomForestClassifier(n_jobs=-1)\n",
    "lg_results = cross_validate(lgfit, X, Y.binary_VP, cv=5,\n",
    "                         scoring=scoring)\n",
    "lg_results = pd.DataFrame.from_dict(lg_results)\n",
    "results = pd.DataFrame()\n",
    "results['mean'] = lg_results.drop(['fit_time', 'score_time'], axis=1).mean()\n",
    "results['se'] = lg_results.drop(['fit_time', 'score_time'], axis=1).sem()\n",
    "results= results.T\n",
    "results['model'] = modelname\n",
    "cv_results =cv_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf-base+\n",
    "modelname = 'RF-base+'\n",
    "lgfit = RandomForestClassifier(n_jobs=-1)\n",
    "lg_results = cross_validate(lgfit, X_extra, Y.binary_VP, cv=5,\n",
    "                         scoring=scoring)\n",
    "lg_results = pd.DataFrame.from_dict(lg_results)\n",
    "results = pd.DataFrame()\n",
    "results['mean'] = lg_results.drop(['fit_time', 'score_time'], axis=1).mean()\n",
    "results['se'] = lg_results.drop(['fit_time', 'score_time'], axis=1).sem()\n",
    "results= results.T\n",
    "results['model'] = modelname\n",
    "cv_results =cv_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb-base\n",
    "modelname = 'XGB-base'\n",
    "lgfit = xgb.XGBClassifier(n_jobs=-1, eval_metric='logloss')\n",
    "lg_results = cross_validate(lgfit, X, Y.binary_VP, cv=5,\n",
    "                         scoring=scoring)\n",
    "lg_results = pd.DataFrame.from_dict(lg_results)\n",
    "results = pd.DataFrame()\n",
    "results['mean'] = lg_results.drop(['fit_time', 'score_time'], axis=1).mean()\n",
    "results['se'] = lg_results.drop(['fit_time', 'score_time'], axis=1).sem()\n",
    "results= results.T\n",
    "results['model'] = modelname\n",
    "cv_results =cv_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'XGB-base+'\n",
    "lgfit = xgb.XGBClassifier(n_jobs=-1, eval_metric='logloss')\n",
    "lg_results = cross_validate(lgfit, X_extra, Y.binary_VP, cv=5,\n",
    "                         scoring=scoring)\n",
    "lg_results = pd.DataFrame.from_dict(lg_results)\n",
    "results = pd.DataFrame()\n",
    "results['mean'] = lg_results.drop(['fit_time', 'score_time'], axis=1).mean()\n",
    "results['se'] = lg_results.drop(['fit_time', 'score_time'], axis=1).sem()\n",
    "results= results.T\n",
    "results['model'] = modelname\n",
    "cv_results =cv_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save VP Cross validation results\n",
    "cv_results.to_csv('../doseprediction/Final_VP_CV_performance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some stats about the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total features in base model: ', X_train.shape[1])\n",
    "print('total features in base+ model: ', X_train_extra.shape[1])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
