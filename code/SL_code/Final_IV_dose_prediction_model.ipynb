{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV fluid dose prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import xgboost as xgb \n",
    "from sklearn.linear_model import  LogisticRegression, LinearRegression\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt \n",
    "import mlxtend\n",
    "import imblearn \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import  cross_validate, cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputed \n",
    "traindata = pd.read_csv('../doseprediction/data/sl_train.csv')\n",
    "testdata = pd.read_csv('../doseprediction/data/sl_val.csv')\n",
    "print(traindata.shape)\n",
    "print(testdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfeatures = ['Gender','Ventilator', 'diabetes', 'metastatic_cancer', 'qsofa_sysbp_score',\n",
    "       'qsofa_resprate_score', 'qsofa_gcs_score', \n",
    "        'Age', 'Weight', 'Height', 'bmi', 'ANION_GAP',\n",
    "       'APTT', 'Albumine', 'Art_BE', 'Art_PH',  'Bicarbonaat',\n",
    "       'Calcium', 'Chloride', 'DIA', 'FiO2', 'Glucose', 'HB', 'HeartRate',\n",
    "       'Ion_Ca', 'Kalium', 'LEU', 'Lactate', 'MAP', 'Magnesium', 'Natrium',\n",
    "       'PF_ratio', 'PaCO2', 'PaO2', 'RespRate', 'SYS', 'Shock_Index',\n",
    "       'Sirs_score', 'Sofa_score', 'Temp', 'Trombo', 'elixhauser',\n",
    "       'elixhauser_hospital', 'qsofa', 'mingcs', 'lods', \n",
    "       'SpO2', 'Ureum', 'Creat', 'ALAT',\n",
    "       'ASAT', 'Bili', 'INR' ]# 'blood_culture_positive','race_white', 'race_black', 'race_hispanic', 'race_other']\n",
    "\n",
    "fluidfeatures = ['cumm_fluid_balance','total_IV_prev', 'max_VP_prev', \n",
    "                            'Running_total_UP', 'total_UP']\n",
    "\n",
    "target = ['max_VP', 'total_IV', 'discrete_action', 'discrete_VP', \n",
    "          'discrete_IV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training predictors\n",
    "X_train = traindata[keyfeatures]\n",
    "X_test = testdata[keyfeatures]\n",
    "X_train_extra =traindata[keyfeatures+fluidfeatures]\n",
    "X_test_extra =testdata[keyfeatures+fluidfeatures]\n",
    "\n",
    "# training targets \n",
    "Y_train = traindata[target]\n",
    "Y_test = testdata[target]\n",
    "#binarize VP as it is highly imbalance \n",
    "Y_train['binary_VP']=  np.where(Y_train.discrete_VP==0, 0, 1)\n",
    "Y_test = testdata[target]\n",
    "Y_test['binary_VP']=  np.where(Y_test.discrete_VP==0, 0, 1)\n",
    "\n",
    "# Y all  for visualization \n",
    "Y = pd.concat([Y_train, Y_test])\n",
    "X = pd.concat([X_train, X_test])\n",
    "X_extra = pd.concat([X_train_extra, X_test_extra])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build train test spli model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regression(model, x_train, y_train, x_test, y_test, modelname):\n",
    "    model.fit(x_train, y_train)\n",
    "    predict = model.predict(x_test)\n",
    "    mae = metrics.mean_absolute_error(y_test, predict)\n",
    "    rmse = metrics.mean_squared_error(y_test, predict, squared = False)\n",
    "    r2 = metrics.r2_score(y_test, predict)\n",
    "    eval = {'ModelName' : modelname,\n",
    "            'MAE' : mae,\n",
    "            'RMSE' : rmse,\n",
    "            'R2' : r2}\n",
    "    return model, pd.DataFrame(eval, index=[0])\n",
    "#dataframe to store model performances\n",
    "performance = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression models with base features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression base model\n",
    "modelname='LM-base' \n",
    "model1 = LinearRegression()\n",
    "model1, per = eval_regression(model1, X_train, Y_train.total_IV,\n",
    "                            X_test, Y_test.total_IV, modelname )\n",
    "performance= performance.append(per)\n",
    "\n",
    "#RandomForest base model\n",
    "modelname='RF-base' \n",
    "model2 = RandomForestRegressor(random_state=112, n_jobs=-1)\n",
    "model2, per = eval_regression(model2, X_train, Y_train.total_IV,\n",
    "                            X_test, Y_test.total_IV, modelname )\n",
    "performance= performance.append(per)\n",
    "\n",
    "#XGB-base model \n",
    "modelname='XGB-base' \n",
    "model3 = xgb.XGBRegressor(random_state=112, n_jobs=-1, eval_metric='rmse')\n",
    "model3, per = eval_regression(model3, X_train, Y_train.total_IV,\n",
    "                            X_test, Y_test.total_IV, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression model with base+ features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression base model\n",
    "modelname='LM-base+' \n",
    "model4 = LinearRegression()\n",
    "model4, per = eval_regression(model4, X_train_extra, Y_train.total_IV,\n",
    "                            X_test_extra, Y_test.total_IV, modelname )\n",
    "performance= performance.append(per)\n",
    "\n",
    "#RandomForest base model\n",
    "modelname='RF-base+' \n",
    "model5 = RandomForestRegressor(random_state=112, n_jobs=-1)\n",
    "model5, per = eval_regression(model5, X_train_extra, Y_train.total_IV,\n",
    "                            X_test_extra, Y_test.total_IV, modelname )\n",
    "performance= performance.append(per)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB-base model \n",
    "modelname='XGB-base+' \n",
    "model6 = xgb.XGBRegressor(random_state=112, n_jobs=-1, eval_metric='rmse')\n",
    "model6, per = eval_regression(model6, X_train_extra, Y_train.total_IV,\n",
    "                            X_test_extra, Y_test.total_IV, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB-base model optimal parameters\n",
    "modelname='XGB-base+optimize' \n",
    "model7 = xgb.XGBRegressor(random_state=112, n_jobs=-1,\n",
    "                             eval_metric='rmse', n_estimators=271,\n",
    "                             min_child_weight=5, max_depth=8, gamma=1,\n",
    "                             colsample_bytree=0.96)\n",
    "model7, per = eval_regression(model7, X_train_extra, Y_train.total_IV,\n",
    "                            X_test_extra, Y_test.total_IV, modelname )\n",
    "performance= performance.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Significance test RF and XGB model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compare results based on paired test \n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "### Compare RF-base + (model 5) and XGB base + (model 6)  \n",
    "t,p = paired_ttest_5x2cv(estimator1=model5,\n",
    "                          estimator2=model6,\n",
    "                          X=X_extra, y=Y.total_IV,\n",
    "                          random_seed=112, \n",
    "                          scoring=make_scorer(metrics.mean_squared_error))\n",
    "\n",
    "print('stat:', t)\n",
    "print('p-value:', p)\n",
    "\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0), \\\n",
    "    and may conclude that the performance of the two algorithms is not significantly different.')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0), model are significantly different')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boot strap model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_values = []\n",
    "rmse_values = []\n",
    "mae_values = []\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = X_extra\n",
    "df['total_IV'] = Y.total_IV\n",
    "n_iteration = 80\n",
    "n_size = int(len(X)) * 0.80\n",
    "stats = list()\n",
    "for i in range(n_iteration):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_extra, Y.total_IV, test_size=0.20,\n",
    "                                           random_state=0+i)\n",
    "    \n",
    "    model = xgb.XGBRegressor(eval_metric='rmse', n_jobs=-1)\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction=model.predict(x_test)\n",
    "    r2 = metrics.r2_score(y_test, prediction )\n",
    "    rmse = metrics.mean_squared_error(y_test, prediction, squared=False)\n",
    "    mae = metrics.mean_absolute_error(y_test, prediction)\n",
    "    #append metrices\n",
    "    r2_values.append(r2)\n",
    "    mae_values.append(mae)\n",
    "    rmse_values.append(rmse)\n",
    "    \n",
    "\n",
    "print('Final average accuracy',np.mean(r2_values))\n",
    "print('Final average rmse',np.mean(rmse_values))\n",
    "print('Final average mae',np.mean(mae_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rmse_values)\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.show()\n",
    "plt.hist(r2_values)\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets find Confidence intervals\n",
    "stats= r2_values\n",
    "a = 0.95 # for 95% confidence\n",
    "p = ((1.0 - a)/2.0) * 100 #tail regions on right and left .25 on each side indicated by P value (border)\n",
    "                          # 1.0 is total area of this curve, 2.0 is actually .025 thats the space we would want to be \n",
    "                            #left on either side\n",
    "lower = max(0.0, np.percentile(stats,p))\n",
    "\n",
    "p = (a + ((1.0 - a)/ 2.0)) * 100 #p is limits\n",
    "upper = min(1.0, np.percentile(stats,p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' %(a*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save bootstrap results \n",
    "iv_bootstrap = {'r2_boot': r2_values,\n",
    "                'mae_boot': mae_values,\n",
    "                'rmse_boot' : rmse_values}\n",
    "import joblib\n",
    "joblib.dump(iv_bootstrap, '../doseprediction/Final_IV_bootstrap_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyper parameter tuning the Final best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials , fmin, hp, tpe\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 9, 2),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': hp.randint('n_estimators', 50, 300)\n",
    "    }\n",
    "\n",
    "\n",
    "def hyperparameter_tuning_reg(space):\n",
    "    x_train = X_train_extra\n",
    "    y_train = Y_train.total_IV\n",
    "    x_test = X_test_extra\n",
    "    y_test = Y_test.total_IV\n",
    "    model=xgb.XGBRegressor(n_estimators =space['n_estimators'], \n",
    "                            max_depth = int(space['max_depth']), \n",
    "                          gamma = space['gamma'],\n",
    "                         min_child_weight=space['min_child_weight'],\n",
    "                         colsample_bytree=space['colsample_bytree'])\n",
    "    \n",
    "    evaluation = [( x_train, y_train), ( x_test, y_test)]\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "\n",
    "    pred = model.predict(x_test)\n",
    "    mse= metrics.mean_squared_error(y_test, pred, squared=False)\n",
    "    r2_score= metrics.r2_score(y_test, pred)\n",
    "    print('r2 ', r2_score, 'rmse ', mse)\n",
    "    return {'loss':mse, 'r2_score':r2_score, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "iv_trials = Trials()\n",
    "best_iv_params =  fmin(fn=hyperparameter_tuning_reg,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50,\n",
    "            trials=iv_trials)\n",
    "\n",
    "print('The best parameters for total IV prediction are', '\\n')\n",
    "print(best_iv_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checked but optimize model does not give best performance "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
