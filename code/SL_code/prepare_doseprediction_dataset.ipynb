{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import os \n",
    "import copy\n",
    "from fancyimpute import KNN \n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_data = pd.read_csv('../commondata/MIMIC_data_allbins.csv')\n",
    "print('total row and columns in MIMIC data', MIMIC_data.shape)\n",
    "print('total patient in the file ', MIMIC_data.icustay_id.value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize action values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(df: pd.DataFrame, col1: str, col2: str):\n",
    "    \"\"\"\n",
    "    Given a dataframe with at least\n",
    "    two categorical columns, create a \n",
    "    confusion matrix of the count of the columns\n",
    "    cross-counts\n",
    "    \n",
    "    use like:\n",
    "    \n",
    "    >>> confusion_matrix(test_df, 'actual_label', 'predicted_label')\n",
    "    \"\"\"\n",
    "    return (\n",
    "            df\n",
    "            .groupby([col1, col2])\n",
    "            .size()\n",
    "            .unstack(fill_value=0)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = MIMIC_data[MIMIC_data.dbsource=='carevue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(k2, 'discrete_IV', 'discrete_VP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geting the quartile only based on metavision for IV and Vassopressor \n",
    "vp_quartile = pd.qcut(MIMIC_data.loc[(MIMIC_data.dbsource=='metavision') &(MIMIC_data['max_VP']!=0)]['max_VP'], q=4)\n",
    "vp_quartile_bins = [-float(\"inf\"), 0.009, 0.11, 0.225, 0.451, float(\"inf\")] # I have checke dand this is the bin \n",
    "#discreteze vassopressor action based on values \n",
    "vp_discrete = pd.cut(MIMIC_data.max_VP, bins = [-float(\"inf\"), 0.009, 0.11, 0.225, 0.451, float(\"inf\")], labels = [0, 1, 2, 3, 4])\n",
    "\n",
    "##get quartile for IV data \n",
    "iv_quartile =  pd.qcut(MIMIC_data.loc[(MIMIC_data.dbsource=='metavision') &(MIMIC_data['total_IV']!=0)]['total_IV'], q=4)\n",
    "iv_quartile_bins = [-float(\"inf\"), 0, 100, 292, 753, float(\"inf\")] # putting infinity to define lower and upper bound categories \n",
    "#discretize IV \n",
    "iv_discrete = pd.cut(MIMIC_data.total_IV, bins = [-float(\"inf\"), 0, 100, 292, 753, float(\"inf\")], labels = [0, 1, 2, 3, 4])\n",
    "\n",
    "### discretize action space \n",
    "action = vp_discrete.cat.codes * 5 + iv_discrete.cat.codes\n",
    "\n",
    "## Replace value in MIMIC dataset \n",
    "MIMIC_data['discrete_IV'] = iv_discrete.cat.codes\n",
    "MIMIC_data['discrete_VP'] = vp_discrete.cat.codes\n",
    "MIMIC_data['discrete_action'] = action.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### confusion matrix\n",
    "# #carevue  \n",
    "confusion_matrix(MIMIC_data[MIMIC_data.dbsource=='carevue'], 'discrete_IV', 'discrete_VP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### confusion matrix\n",
    "# #metavision \n",
    "confusion_matrix(MIMIC_data[MIMIC_data.dbsource=='metavision'], 'discrete_IV', 'discrete_VP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define features based on which we calculate missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_features = ['Gender','Ventilator', #binary features\n",
    "                'Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "                 'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "                 'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "                 'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "                 'PF_ratio','Albumine', 'Ion_Ca', # normal. features\n",
    "                'max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR', #logfeatures\n",
    "                'Running_total_IV','total_IV_prev','Running_total_UP','total_UP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(MIMIC_data.isna().sum()/MIMIC_data.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove low resolution patients data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get columns which contains missing value \n",
    "miss_values= MIMIC_data[dss_features].isna().sum(axis=0).sort_values(ascending=False)\n",
    "misscols = miss_values[miss_values>0].index\n",
    "#filter \n",
    "MIMIC_data['Row_wise_missing'] = MIMIC_data[misscols].isna().sum(axis=1)/MIMIC_data[misscols].shape[1] * 100 #MIMIC_data.isna().sum(axis=1)/MIMIC_data.shape[1] * 100\n",
    "#get miniumm traj % patient \n",
    "min_percent_missing_ptlist = MIMIC_data.loc[MIMIC_data.groupby('icustay_id')['Row_wise_missing'].idxmin()][['icustay_id','Row_wise_missing']]\n",
    "discard_min_pt_list = min_percent_missing_ptlist[min_percent_missing_ptlist['Row_wise_missing']>20].icustay_id.tolist()\n",
    "mean_NA_per_patient = MIMIC_data.groupby(['icustay_id'])['Row_wise_missing'].mean()\n",
    "discard_mean_pt_list  = mean_NA_per_patient.index[mean_NA_per_patient>70].unique().tolist()\n",
    "exclude_ptid_list = discard_min_pt_list+discard_mean_pt_list\n",
    "print('total icustay to discard ', len(set(exclude_ptid_list)))\n",
    "MIMIC_data = MIMIC_data[~MIMIC_data.icustay_id.isin(exclude_ptid_list)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop trajectories where 80 % of key features are missing \n",
    "MIMIC_data = MIMIC_data[MIMIC_data.Row_wise_missing < 80].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby farward fill \n",
    "MIMIC_data[dss_features]= MIMIC_data.groupby('icustay_id')[dss_features].transform(lambda x: x.ffill())\n",
    "#discard row miss columns\n",
    "MIMIC_data = MIMIC_data.drop(['Row_wise_missing'], axis=1).reset_index(drop=True)\n",
    "## Remove patients which contains less than 3 trajectories \n",
    "trajcount = MIMIC_data.icustay_id.value_counts()\n",
    "discard_p = trajcount[trajcount<3] #no traject to discard here \n",
    "len(discard_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove care vue patients with action space 5, 10, 15, 20 \n",
    "#only select patinet who does not have 5,10, 15, 20 action space (just like MV data)\n",
    "carevue_id_to_discard = MIMIC_data.loc[MIMIC_data.discrete_action.isin([5,10,15,20]) & (MIMIC_data.dbsource=='carevue') ]['icustay_id'].value_counts()\n",
    "print('total carevue discard', len(carevue_id_to_discard))\n",
    "MIMIC_data = MIMIC_data[~MIMIC_data.icustay_id.isin(carevue_id_to_discard.index)].reset_index(drop=True)\n",
    "MIMIC_data.icustay_id.value_counts().shape #16512 patients to include in the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cap values and get cummulative balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = pd.read_csv('../commondata/capping_values.csv', sep=',',decimal='.')\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for i in caps.index:\n",
    "        param = caps.loc[i,'Parameter']\n",
    "        maxval = caps.loc[i,'maxval']\n",
    "        minval = caps.loc[i,'minval']\n",
    "        #print(param,minval,maxval)\n",
    "        MIMIC_data[param][MIMIC_data[param] >= maxval] = maxval\n",
    "        MIMIC_data[param][MIMIC_data[param] <= minval] = minval\n",
    "\n",
    "#calculate fluid balance by subtracting IV - UP \n",
    "MIMIC_data['cumm_fluid_balance'] = MIMIC_data['total_IV']-MIMIC_data['total_UP']\n",
    "## Recalculate BMI \n",
    "MIMIC_data['bmi'] = MIMIC_data.Weight / (MIMIC_data.Height/100)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_data.to_csv('../commondata/MIMIC_data_clean_traj.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train test split data set using icustay as group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_1 = GroupShuffleSplit(n_splits=1, train_size=.80, random_state=42)\n",
    "train_ix, val_ix = next(gs_1.split(MIMIC_data, groups=MIMIC_data.icustay_id))\n",
    "train_set = MIMIC_data.iloc[train_ix].reset_index(drop=True)\n",
    "val_set = MIMIC_data.iloc[val_ix].reset_index(drop=True)\n",
    "train_ids = train_set.icustay_id.value_counts().shape[0]\n",
    "val_ids = val_set.icustay_id.value_counts().shape[0]\n",
    "\n",
    "print('Total patient in trainset is ', train_ids)\n",
    "print('Total patient in validationset is ', val_ids)\n",
    "\n",
    "print('Total patient in trainset is ', train_ids, ' total states ', train_set.shape[0])\n",
    "print('Total patient in valset is ', val_ids, ' total states', val_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a raw data copy\n",
    "train_rawdata = train_set.copy()\n",
    "val_rawdata = val_set.copy()\n",
    "\n",
    "print(train_rawdata.shape)\n",
    "print(val_rawdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the features for Transformation/Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_fields = ['Gender','Ventilator', \n",
    "                    'qsofa_gcs_score', \n",
    "                    'qsofa_resprate_score',\n",
    "                     'qsofa_sysbp_score',\n",
    "                     'diabetes', 'metastatic_cancer']\n",
    "\n",
    "norm_fields= ['Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "    'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "    'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "    'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "    'PF_ratio','Albumine', 'Ion_Ca', \n",
    "    'mingcs', 'lods', 'elixhauser', 'cumm_fluid_balance', 'bmi', 'qsofa'] \n",
    "log_fields = ['max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR',\n",
    "              'Running_total_IV','total_IV_prev','Running_total_UP','total_UP', 'total_IV','max_VP']\n",
    "\n",
    "not_used = ['subject_id', 'hadm_id', 'icustay_id', 'interval_start_time',\n",
    "            'interval_end_time', 'Reward',\n",
    "             'Discharge', 'discrete_action', \n",
    "             'morta_90', 'morta_hosp',  're_admission', \n",
    "              'composite_outcome', 'action', 'dbsource', 'exclude',\n",
    "              'blood_culture_positive', \n",
    "              'race_black', 'race_hispanic', 'race_other', 'race_white',\n",
    "              're_admission', 'elixhauser_hospital',\n",
    "               'BANDS',  'discrete_IV', 'discrete_VP', 'discrete_action_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfeatures = not_used+binary_fields+norm_fields+log_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise binary fields\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_set[binary_fields] = train_set[binary_fields] - 0.5 \n",
    "    val_set[binary_fields] = val_set[binary_fields] - 0.5 \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal distn fields\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for item in norm_fields:\n",
    "        av = train_set[item].mean()\n",
    "        std = train_set[item].std()\n",
    "        train_set[item] = (train_set[item] - av) / std\n",
    "        val_set[item] = (val_set[item] - av) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    # log normal fields\n",
    "    train_set[log_fields] = np.log(0.1 + train_set[log_fields])\n",
    "    val_set[log_fields] = np.log(0.1 + val_set[log_fields])\n",
    "    \n",
    "    \n",
    "    for item in log_fields:\n",
    "        av = train_set[item].mean()\n",
    "        std = train_set[item].std()\n",
    "        train_set[item] = (train_set[item] - av) / std\n",
    "        val_set[item] = (val_set[item] - av) / std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# scale all features\n",
    "scalable_fields = copy.deepcopy(binary_fields)\n",
    "scalable_fields.extend(norm_fields)\n",
    "scalable_fields.extend(log_fields)\n",
    "\n",
    "# min-max normalization\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for col in scalable_fields:\n",
    "        minimum = np.nanmin(train_set[col])\n",
    "        maximum = np.nanmax(train_set[col])\n",
    "        #print(col,minimum,maximum)\n",
    "        train_set[col] = (train_set[col] - minimum)/(maximum-minimum)\n",
    "        val_set[col] = (val_set[col] - minimum)/(maximum-minimum)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute data with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN \n",
    "def knn_impute(df1, featureset):\n",
    "    df = df1.copy()\n",
    "    impute_before = (df[featureset].isna().sum()/df[featureset].shape[0]).mean()\n",
    "    print('total avearge missing before imputation ', impute_before)\n",
    "    df_unique_ids = df['icustay_id'].unique()\n",
    "    for unique_id in df_unique_ids:\n",
    "        X_incomplete = df.loc[df['icustay_id']==unique_id][featureset]\n",
    "        pd.reset_option('mode.chained_assignment')\n",
    "        with pd.option_context('mode.chained_assignment', None):\n",
    "            df.loc[df['icustay_id']==unique_id,featureset] = KNN(k=3,verbose=False).fit_transform(X_incomplete)\n",
    "    impute_after = (df[featureset].isna().sum()/df[featureset].shape[0]).mean()\n",
    "    print('total avearge missing after imputation ', impute_after)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute data based on Default state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputefeatures = binary_fields+norm_fields+log_fields\n",
    "print('totalfeatures to be imputed', len(imputefeatures))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_set_imputed = knn_impute(df1=train_set.copy(), featureset= imputefeatures)\n",
    "    val_set_imputed = knn_impute(df1=val_set.copy(), featureset= imputefeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save raw data to csv files\n",
    "#train_rawdata.to_csv('../doseprediction/data/sl_train_rawdata.csv', index=False)\n",
    "#val_rawdata.to_csv('../doseprediction/data/sl_val_rawdata.csv', index=False)\n",
    "#pd.Series(allfeatures).to_csv('../doseprediction/data/sl_all_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save processed data to csv files (this contain 18 state per patient)\n",
    "train_set_imputed.to_csv('../doseprediction/data/sl_train.csv', index=False)\n",
    "val_set_imputed.to_csv('../doseprediction/data/sl_val.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
