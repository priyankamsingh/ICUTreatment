{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import os \n",
    "import copy\n",
    "from fancyimpute import KNN \n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_data = pd.read_csv('../../MIMIC_data_allbins.csv')\n",
    "print('total row and columns in MIMIC data', MIMIC_data.shape)\n",
    "print('total patient in the file ', MIMIC_data.icustay_id.value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize action (There was some confusion on quartile range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geting the quartile only based on metavision for IV and Vassopressor \n",
    "vp_quartile = pd.qcut(MIMIC_data.loc[(MIMIC_data.dbsource=='metavision') &(MIMIC_data['max_VP']!=0)]['max_VP'], q=4)\n",
    "vp_quartile_bins = [-float(\"inf\"), 0.009, 0.11, 0.225, 0.451, float(\"inf\")] # I have checke dand this is the bin \n",
    "#discreteze vassopressor action based on values \n",
    "vp_discrete = pd.cut(MIMIC_data.max_VP, bins = [-float(\"inf\"), 0.009, 0.11, 0.225, 0.451, float(\"inf\")], labels = [0, 1, 2, 3, 4])\n",
    "\n",
    "##get quartile for IV data \n",
    "iv_quartile =  pd.qcut(MIMIC_data.loc[(MIMIC_data.dbsource=='metavision') &(MIMIC_data['total_IV']!=0)]['total_IV'], q=4)\n",
    "iv_quartile_bins = [-float(\"inf\"), 0, 100, 292, 753, float(\"inf\")] # putting infinity to define lower and upper bound categories \n",
    "#discretize IV \n",
    "iv_discrete = pd.cut(MIMIC_data.total_IV, bins = [-float(\"inf\"), 0, 100, 292, 753, float(\"inf\")], labels = [0, 1, 2, 3, 4])\n",
    "\n",
    "### discretize action space \n",
    "action = vp_discrete.cat.codes * 5 + iv_discrete.cat.codes\n",
    "\n",
    "## Replace value in MIMIC dataset \n",
    "MIMIC_data['discrete_IV'] = iv_discrete.cat.codes\n",
    "MIMIC_data['discrete_VP'] = vp_discrete.cat.codes\n",
    "MIMIC_data['discrete_action'] = action.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "plt.scatter(np.log10(MIMIC_data.total_IV), np.log10(MIMIC_data.max_VP), c=MIMIC_data.discrete_action)\n",
    "plt.ylabel('Vassopressor dose (log)')\n",
    "plt.xlabel('IV fluid dose (log)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing low resolution patients data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_features = ['Gender','Ventilator', #binary features\n",
    "                'Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "                 'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "                 'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "                 'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "                 'PF_ratio','Albumine', 'Ion_Ca', # normal. features\n",
    "                'max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR', #logfeatures\n",
    "                'Running_total_IV','total_IV_prev','Running_total_UP','total_UP',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dss_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get columns which contains missing value \n",
    "miss_values= MIMIC_data[dss_features].isna().sum(axis=0).sort_values(ascending=False)\n",
    "misscols = miss_values[miss_values>0].index\n",
    "#filter \n",
    "MIMIC_data['Row_wise_missing'] = MIMIC_data[misscols].isna().sum(axis=1)/MIMIC_data[misscols].shape[1] * 100 #MIMIC_data.isna().sum(axis=1)/MIMIC_data.shape[1] * 100\n",
    "#get miniumm traj % patient \n",
    "min_percent_missing_ptlist = MIMIC_data.loc[MIMIC_data.groupby('icustay_id')['Row_wise_missing'].idxmin()][['icustay_id','Row_wise_missing']]\n",
    "discard_min_pt_list = min_percent_missing_ptlist[min_percent_missing_ptlist['Row_wise_missing']>20].icustay_id.tolist()\n",
    "mean_NA_per_patient = MIMIC_data.groupby(['icustay_id'])['Row_wise_missing'].mean()\n",
    "discard_mean_pt_list  = mean_NA_per_patient.index[mean_NA_per_patient>70].unique().tolist()\n",
    "exclude_ptid_list = discard_min_pt_list+discard_mean_pt_list\n",
    "print('total icustay to discard ', len(set(exclude_ptid_list)))\n",
    "MIMIC_data = MIMIC_data[~MIMIC_data.icustay_id.isin(exclude_ptid_list)].drop(['Row_wise_missing'], axis=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Forward fill group wise\n",
    "MIMIC_data[dss_features] = MIMIC_data.groupby('icustay_id')[dss_features].transform(lambda x: x.ffill())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_data[MIMIC_data.dbsource=='metavision']['icustay_id'].value_counts().shape[0] # 7359 for the data modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cap values and get cummulative balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caps = pd.read_csv('../../capping_values.csv', sep=',',decimal='.')\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for i in caps.index:\n",
    "        param = caps.loc[i,'Parameter']\n",
    "        maxval = caps.loc[i,'maxval']\n",
    "        minval = caps.loc[i,'minval']\n",
    "        #print(param,minval,maxval)\n",
    "        MIMIC_data[param][MIMIC_data[param] >= maxval] = maxval\n",
    "        MIMIC_data[param][MIMIC_data[param] <= minval] = minval\n",
    "\n",
    "#calculate fluid balance by subtracting IV - UP \n",
    "MIMIC_data['cumm_fluid_balance'] = MIMIC_data['total_IV']-MIMIC_data['total_UP']\n",
    "#MIMIC_data['cumm_fluid_balance'] = MIMIC_data.groupby(\"icustay_id\")['cumm_fluid_balance'].shift(+1)\n",
    "#MIMIC_data['cumm_fluid_balance'] = MIMIC_data['cumm_fluid_balance'].fillna(0)\n",
    "## Recalculate BMI \n",
    "MIMIC_data['bmi'] = MIMIC_data.Weight / (MIMIC_data.Height/100)**2\n",
    "\n",
    "# get only metavision data \n",
    "MIMIC_MVdata = MIMIC_data[MIMIC_data.dbsource=='metavision'].reset_index(drop=True)\n",
    "## fixing action space asin MV only 21 actions exist \n",
    "# actual (fancy-ass) mapping function\n",
    "mapping = [x for x in range(25) if x not in [5, 10, 15, 20]]\n",
    "MIMIC_MVdata['discrete_action_original'] = MIMIC_MVdata['discrete_action'].copy()\n",
    "# apply mapping\n",
    "MIMIC_MVdata['discrete_action'] = MIMIC_MVdata['discrete_action'].apply(lambda x: mapping.index(x))\n",
    "print('total patients in Metavision datset', MIMIC_MVdata.icustay_id.value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparae a Evaluation set based on Carevue data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a testset based on carevue\n",
    "#only select patinet who does not have 5,10, 15, 20 action space (just like MV data)\n",
    "carevue_id_to_keep = MIMIC_data.loc[~MIMIC_data.discrete_action.isin([5,10,15,20]) & (MIMIC_data.dbsource=='carevue') ]['icustay_id'].value_counts()\n",
    "evaluation_set = MIMIC_data[MIMIC_data.icustay_id.isin(carevue_id_to_keep[carevue_id_to_keep>=18].index)] \n",
    "## fix the action space \n",
    "# actual (fancy-ass) mapping function\n",
    "mapping = [x for x in range(25) if x not in [5, 10, 15, 20]]\n",
    "evaluation_set['discrete_action_original'] = evaluation_set['discrete_action'].copy()\n",
    "# apply mapping\n",
    "evaluation_set['discrete_action'] = evaluation_set['discrete_action'].apply(lambda x: mapping.index(x))\n",
    "print('total patients in carevue evaluation set', evaluation_set.icustay_id.value_counts().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split data set using icustay as group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "####### MIMIC SPLIT\n",
    "# now split into train/validation/test sets\n",
    "unique_ids = MIMIC_MVdata['icustay_id'].unique()\n",
    "\n",
    "random.shuffle(unique_ids)\n",
    "train_sample = 0.80\n",
    "train_num = int(len(unique_ids) * train_sample)\n",
    "train_ids = unique_ids[:train_num]\n",
    "val_ids = unique_ids[train_num:]\n",
    "# Create datasets\n",
    "train_set = MIMIC_MVdata.loc[MIMIC_MVdata['icustay_id'].isin(train_ids)]\n",
    "val_set = MIMIC_MVdata.loc[MIMIC_MVdata['icustay_id'].isin(val_ids)]\n",
    "test_set = evaluation_set.copy()\n",
    "train_ids = train_set.icustay_id.value_counts().shape[0]\n",
    "val_ids = val_set.icustay_id.value_counts().shape[0]\n",
    "test_ids = test_set.icustay_id.value_counts().shape[0]\n",
    "print('Total patient in trainset is ', train_ids)\n",
    "print('Total patient in validationset is ', val_ids)\n",
    "print('Total patient in testset is ', test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a raw data copy\n",
    "train_rawdata = train_set.copy()\n",
    "val_rawdata = val_set.copy()\n",
    "test_rawdata = test_set.copy()\n",
    "\n",
    "print(train_rawdata.shape)\n",
    "print(val_rawdata.shape)\n",
    "print(test_rawdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Features For Transformation (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_fields = ['Gender','Ventilator', 'qsofa_gcs_score', 'qsofa_resprate_score', 'qsofa_sysbp_score']\n",
    "\n",
    "norm_fields= ['Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "    'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "    'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "    'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "    'PF_ratio','Albumine', 'Ion_Ca', \n",
    "    'mingcs', 'lods', 'elixhauser', 'cumm_fluid_balance', 'bmi', 'qsofa'] \n",
    "log_fields = ['max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR',\n",
    "              'Running_total_IV','total_IV_prev','Running_total_UP','total_UP']\n",
    "\n",
    "not_used = ['subject_id', 'hadm_id', 'icustay_id', 'interval_start_time',\n",
    "            'interval_end_time', 'Reward',\n",
    "             'Discharge', 'discrete_action','total_IV','max_VP', \n",
    "             'morta_90', 'morta_hosp',  're_admission', \n",
    "              'composite_outcome', 'action', 'dbsource', 'exclude',\n",
    "              'blood_culture_positive', 'metastatic_cancer',\n",
    "              'race_black', 'race_hispanic', 'race_other', 'race_white',\n",
    "              're_admission', 'elixhauser_hospital',\n",
    "               'BANDS', 'diabetes', 'discrete_IV', 'discrete_VP', 'discrete_action_original']\n",
    "\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise binary fields\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_set[binary_fields] = train_set[binary_fields] - 0.5 \n",
    "    val_set[binary_fields] = val_set[binary_fields] - 0.5 \n",
    "    test_set[binary_fields] = test_set[binary_fields] - 0.5\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal distn fields\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for item in norm_fields:\n",
    "        av = train_set[item].mean()\n",
    "        std = train_set[item].std()\n",
    "        train_set[item] = (train_set[item] - av) / std\n",
    "        val_set[item] = (val_set[item] - av) / std\n",
    "        test_set[item] = (test_set[item] - av) / std\n",
    "        #print(item,av.round(3),std.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    # log normal fields\n",
    "    train_set[log_fields] = np.log(0.1 + train_set[log_fields])\n",
    "    val_set[log_fields] = np.log(0.1 + val_set[log_fields])\n",
    "    test_set[log_fields] = np.log(0.1 + test_set[log_fields])\n",
    "    \n",
    "    for item in log_fields:\n",
    "        av = train_set[item].mean()\n",
    "        std = train_set[item].std()\n",
    "        train_set[item] = (train_set[item] - av) / std\n",
    "        val_set[item] = (val_set[item] - av) / std\n",
    "        test_set[item] = (test_set[item] - av) / std\n",
    "        #print(item,av.round(3),std.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# scale all features\n",
    "scalable_fields = copy.deepcopy(binary_fields)\n",
    "scalable_fields.extend(norm_fields)\n",
    "scalable_fields.extend(log_fields)\n",
    "\n",
    "# min-max normalization\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for col in scalable_fields:\n",
    "        minimum = np.nanmin(train_set[col])\n",
    "        maximum = np.nanmax(train_set[col])\n",
    "        #print(col,minimum,maximum)\n",
    "        train_set[col] = (train_set[col] - minimum)/(maximum-minimum)\n",
    "        val_set[col] = (val_set[col] - minimum)/(maximum-minimum)\n",
    "        test_set[col] = (test_set[col] - minimum)/(maximum-minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### impute only considering features from ESS\n",
    "#extended features #extended_binary 'qsofa_gcs_score',  'qsofa_sysbp_score', 'mingcs',  'cumm_fluid_balance'\n",
    "\n",
    "binary_fields = ['Gender','Ventilator', 'qsofa_gcs_score',  'qsofa_sysbp_score'] \n",
    "\n",
    "norm_fields= ['Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "    'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "    'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "    'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "    'PF_ratio','Albumine', 'Ion_Ca',  \n",
    "                'mingcs',  'cumm_fluid_balance'] # extended]\n",
    "\n",
    "log_fields = ['max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR',\n",
    "              'Running_total_IV','total_IV_prev','Running_total_UP','total_UP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create a copy of original scaled train set \n",
    "train_set_orig = train_set.copy()\n",
    "test_set_orig = test_set.copy()\n",
    "val_set_orig = val_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Data with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training imputation\n",
    "train_unique_ids = train_set['icustay_id'].unique()\n",
    "train_before_impute = train_set.head(30)\n",
    "for unique_id in train_unique_ids:\n",
    "    X_incomplete = train_set.loc[train_set['icustay_id']==unique_id][binary_fields+norm_fields+log_fields]\n",
    "    pd.reset_option('mode.chained_assignment')\n",
    "    with pd.option_context('mode.chained_assignment', None):\n",
    "        train_set.loc[train_set['icustay_id']==unique_id,binary_fields+norm_fields+log_fields] = KNN(k=3,verbose=False).fit_transform(X_incomplete) # XX_filled_knn\n",
    "print(\"done\")\n",
    "train_set_after_impute = train_set.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_unique_ids = val_set['icustay_id'].unique()\n",
    "val_before_impute = val_set.head(30)\n",
    "for unique_id in val_unique_ids:\n",
    "    X_incomplete = val_set.loc[val_set['icustay_id']==unique_id][binary_fields+norm_fields+log_fields]\n",
    "    pd.reset_option('mode.chained_assignment')\n",
    "    with pd.option_context('mode.chained_assignment', None):\n",
    "        val_set.loc[val_set['icustay_id']==unique_id,binary_fields+norm_fields+log_fields] = KNN(k=3,verbose=False).fit_transform(X_incomplete) # XX_filled_knn\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_ids = test_set['icustay_id'].unique()\n",
    "test_before_impute = test_set.head(30)\n",
    "for unique_id in test_unique_ids:\n",
    "    X_incomplete = test_set.loc[test_set['icustay_id']==unique_id][binary_fields+norm_fields+log_fields]\n",
    "    pd.reset_option('mode.chained_assignment')\n",
    "    with pd.option_context('mode.chained_assignment', None):\n",
    "        test_set.loc[test_set['icustay_id']==unique_id,binary_fields+norm_fields+log_fields] = KNN(k=3,verbose=False).fit_transform(X_incomplete) # XX_filled_knn\n",
    "print(\"done\")\n",
    "test_after_impute = test_set.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_ids = train_set.icustay_id.value_counts().shape[0]\n",
    "total_test_ids = test_set.icustay_id.value_counts().shape[0]\n",
    "total_val_ids = val_set.icustay_id.value_counts().shape[0]\n",
    "print('Total patient in trainset is ', total_train_ids, ' total states ', train_set.shape[0])\n",
    "print('Total patient in testset is ', total_test_ids, ' total states', test_set.shape[0])\n",
    "print('Total patient in valset is ', total_val_ids, ' total states', val_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding state and next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_next_state_id(df):\n",
    "    ''' this will add row_id and next_row id'''\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['row_id'] = df.index\n",
    "    df['row_id_next'] = np.where(df['icustay_id'].shift(-1) !=df['icustay_id'],\n",
    "                                 df['row_id'], df['row_id']+1)\n",
    "    df['row_id_next'][0] = 1\n",
    "    df['row_id_next'].iloc[-1] = df['row_id'].iloc[-1]\n",
    "    return df    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add state id \n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_set = add_state_next_state_id(train_set)\n",
    "    val_set = add_state_next_state_id(val_set)\n",
    "    test_set = add_state_next_state_id(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature list based on lucas studies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_features = ['Gender','Ventilator', #binary features\n",
    "                'Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "                 'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "                 'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "                 'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "                 'PF_ratio','Albumine', 'Ion_Ca', # normal. features\n",
    "                'max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR', #logfeatures\n",
    "                'Running_total_IV','total_IV_prev','Running_total_UP','total_UP',\n",
    "                'mingcs',  'cumm_fluid_balance', 'qsofa_gcs_score',  'qsofa_sysbp_score'] # extended features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ess_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to create disctionary for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = binary_fields+norm_fields+log_fields\n",
    "def create_RL_data_dict(feature_names):\n",
    "    print('total feature to include in RL model is ', len(feature_names))\n",
    "    #sort the feature IDs \n",
    "    feature_names = np.sort(np.array(feature_names))\n",
    "\n",
    "    feature_df_train = train_set[feature_names]\n",
    "    feature_df_val = val_set[feature_names]\n",
    "    feature_df_test = test_set[feature_names]\n",
    "\n",
    "    v = DictVectorizer(sparse = False)\n",
    "    feature_dict_train = feature_df_train.to_dict('records')\n",
    "    feature_dict_val = feature_df_val.to_dict('records')\n",
    "    feature_dict_test = feature_df_test.to_dict('records')\n",
    "\n",
    "    X_train = v.fit_transform(feature_dict_train)\n",
    "    X_val = v.transform(feature_dict_val)\n",
    "    X_test = v.transform(feature_dict_test)\n",
    "\n",
    "    reward_train = train_set.Reward.values\n",
    "    reward_val = val_set.Reward.values\n",
    "    reward_test = test_set.Reward.values\n",
    "\n",
    "    action_train = train_set.discrete_action.values\n",
    "    action_val = val_set.discrete_action.values\n",
    "    action_test = test_set.discrete_action.values\n",
    "\n",
    "    state_row_id_train       = [int(x) for x in train_set.row_id.values]\n",
    "    next_state_row_id_train  = [int(x) for x in  train_set.row_id_next.values]\n",
    "\n",
    "    state_row_id_val         = [int(x) for x in val_set.row_id.values]\n",
    "    next_state_row_id_val    = [int(x) for x in val_set.row_id_next.values]\n",
    "\n",
    "    state_row_id_test        = [int(x) for x in test_set.row_id.values]\n",
    "    next_state_row_id_test   = [int(x) for x in test_set.row_id_next.values]\n",
    "\n",
    "    output_dict = {'train' : {\n",
    "                        'X' : X_train,\n",
    "                        'action' : action_train,\n",
    "                        'reward' : reward_train,\n",
    "                        'state_id' : state_row_id_train,\n",
    "                        'next_state_id' : next_state_row_id_train\n",
    "                    },\n",
    "                    'val' : {\n",
    "                        'X' : X_val,\n",
    "                        'action' : action_val,\n",
    "                        'reward' : reward_val,\n",
    "                        'state_id' : state_row_id_val,\n",
    "                        'next_state_id' : next_state_row_id_val\n",
    "                    },\n",
    "                'test' : {\n",
    "                        'X' : X_test,\n",
    "                        'action' : action_test,\n",
    "                        'reward' : reward_test,\n",
    "                        'state_id' : state_row_id_test,\n",
    "                        'next_state_id' : next_state_row_id_test\n",
    "                    },\n",
    "                'v' : v,\n",
    "                'featurenames': np.sort(np.array(list(feature_dict_train[1].keys())))\n",
    "         }\n",
    "\n",
    "    print(len(feature_dict_train))\n",
    "    print(len(output_dict['train']['next_state_id']))\n",
    "    print(len(output_dict['val']['next_state_id']))\n",
    "    print(len(output_dict['test']['next_state_id']))\n",
    "    print(output_dict['featurenames'])\n",
    "    #print(feature_names)\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Data for default state space (based on Lucas research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data dict for luca features and extended features \n",
    "data_dict_luca = create_RL_data_dict(ess_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write prepared data in specific folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data to csv files\n",
    "#train_rawdata.to_csv('../dss/data/train_rawdata.csv', index=False)\n",
    "#val_rawdata.to_csv('commondata/val_rawdata.csv', index=False)\n",
    "#test_rawdata.to_csv('commondata/test_rawdata.csv', index=False)\n",
    "\n",
    "# Save processed data to csv files\n",
    "train_set.to_csv('../data/train_data_scaled_imputed.csv', index=False)\n",
    "val_set.to_csv('../data/val_data_scaled_imputed.csv', index=False)\n",
    "test_set.to_csv('../data/test_data_scaled_imputed.csv', index=False)\n",
    "\n",
    "# Save Pickle for modelling\n",
    "joblib.dump(data_dict_luca, os.path.join('../data/', 'ess_data_dict.pkl'))\n",
    "pd.Series(ess_features).to_csv(os.path.join('../data/', 'ess_features.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Prepare Data for Extended space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of extended space features\n",
    "- luca's features ( default state )\n",
    "- + 6 features from supervised model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features selected from supervised model to increase lucas state space \n",
    "# 6 features selected to increase the state space : 'qsofa_gcs_score',  'qsofa_sysbp_score', #extended_binary\n",
    "               # 'mingcs', 'lods', 'elixhauser', 'cumm_fluid_balance'\n",
    "\n",
    "extended_features  = ['Gender','Ventilator', #binary features\n",
    "                  'Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "                 'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "                 'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "                 'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "                 'PF_ratio','Albumine', 'Ion_Ca', # normal. features\n",
    "                'max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR', #logfeatures\n",
    "                'Running_total_IV','total_IV_prev','Running_total_UP','total_UP', \n",
    "                 'qsofa_gcs_score',  'qsofa_sysbp_score', #extended_binary\n",
    "                'mingcs', 'lods', 'elixhauser', 'cumm_fluid_balance' ] # extended_normal \n",
    "\n",
    "print('total number of features in the extended space model', len(extended_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We already have scaled and imputed features so we do not need to re-run th ewhole thing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Csv file from Part1 code (Default space data preparation)\n",
    "train_set = pd.read_csv('commondata/train_data_scaled_imputed.csv')\n",
    "val_set = pd.read_csv('commondata/val_data_scaled_imputed.csv')\n",
    "test_set = pd.read_csv('commondata/test_data_scaled_imputed.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('total ids in trainset ', train_set.icustay_id.unique().shape[0])\n",
    "print('total ids in valset ', val_set.icustay_id.unique().shape[0])\n",
    "print('total ids in testset ', test_set.icustay_id.unique().shape[0]) # only carevue patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data based on supervised extended state space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results for extended features\n",
    "data_dict_extend = create_RL_data_dict(extended_features)\n",
    "joblib.dump(data_dict_extend, os.path.join('../../ess/', 'data/ess_data_dict.pkl'))\n",
    "pd.Series(extended_features).to_csv(os.path.join('../../ess/', 'data/extended_features.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shaped reward implementation as per Raghu but we are not using it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shaped_reward(df):\n",
    "    df['shaped_reward'] = 0 \n",
    "    for i in df.index:\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if df.loc[i, 'icustay_id'] == df.loc[i-1, 'icustay_id']:\n",
    "            sofa_cur = df.loc[i,'Sofa_score']\n",
    "            sofa_prev = df.loc[i-1,'Sofa_score']\n",
    "            lact_cur = df.loc[i,'Lactate']\n",
    "            lact_prev = df.loc[i-1,'Lactate']\n",
    "            reward = 0\n",
    "            if sofa_cur == sofa_prev and sofa_cur != 0:\n",
    "                reward += c0\n",
    "                reward += c1*(sofa_cur-sofa_prev)\n",
    "                reward += c2*np.tanh(lact_cur - lact_prev)\n",
    "                df.loc[i-1,'shaped_reward'] = reward\n",
    "        '''if i % 10000 == 0:\n",
    "            print(i)'''\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0a38587170ce5058530d66ab8d71eef54685962c912a0e111f5dda9b3492382"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('rlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
